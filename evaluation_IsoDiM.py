"""
IsoDiM - Main Model Evaluation Script
======================================
Evaluate the IsoDiM model's generation quality.

Usage:
    python evaluation_IsoDiM.py \\
        --name IsoDiM \\
        --tokenizer_name IsoDiM_Tokenizer_High \\
        --tokenizer_model IsoDiM_Tokenizer_High \\
        --model IsoDiM-DiT-XL \\
        --dataset_name t2m
"""

import os
from os.path import join as pjoin
import torch
import numpy as np
import random
from torch.utils.data import DataLoader
import argparse

from models.Tokenizer import Tokenizer_models
from models.IsoDiM import IsoDiM_models
from utils.evaluators import Evaluators
from utils.datasets import Text2MotionDataset, collate_fn
from utils.eval_utils import evaluation_isodim


def main(args):
    # =========================================================================
    # Seed Configuration
    # =========================================================================
    torch.backends.cudnn.benchmark = False
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    torch.autograd.set_detect_anomaly(True)
    
    # Enable TF32 for faster inference
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    
    # =========================================================================
    # Data Loading
    # =========================================================================
    if args.dataset_name == "t2m":
        data_root = f'{args.dataset_dir}/HumanML3D/'
        dim_pose = 67
    else:
        data_root = f'{args.dataset_dir}/KIT-ML/'
        dim_pose = 64
    
    motion_dir = pjoin(data_root, 'new_joint_vecs')
    text_dir = pjoin(data_root, 'texts')
    max_motion_length = 196
    mean = np.load(pjoin(data_root, 'Mean.npy'))
    std = np.load(pjoin(data_root, 'Std.npy'))
    eval_mean = np.load(f'utils/eval_mean_std/{args.dataset_name}/eval_mean.npy')
    eval_std = np.load(f'utils/eval_mean_std/{args.dataset_name}/eval_std.npy')
    
    split_file = pjoin(data_root, 'test.txt')
    eval_dataset = Text2MotionDataset(
        eval_mean, eval_std, split_file, args.dataset_name, motion_dir, text_dir,
        4, max_motion_length, 20, evaluation=True
    )
    eval_loader = DataLoader(
        eval_dataset, batch_size=32, num_workers=args.num_workers,
        drop_last=True, collate_fn=collate_fn, shuffle=True
    )
    
    # =========================================================================
    # Model Setup
    # =========================================================================
    model_dir = pjoin(args.checkpoints_dir, args.dataset_name, args.name, 'model')
    
    # Load tokenizer
    tokenizer = Tokenizer_models[args.tokenizer_model](input_width=dim_pose)
    ckpt_path = pjoin(
        args.checkpoints_dir, args.dataset_name, args.tokenizer_name, 'model',
        'latest.tar' if args.dataset_name == 't2m' else 'net_best_fid.tar'
    )
    ckpt = torch.load(ckpt_path, map_location='cpu')
    tokenizer.load_state_dict(ckpt['tokenizer'])
    
    # Load IsoDiM model
    ema_isodim = IsoDiM_models[args.model](
        fsq_dim=tokenizer.fsq_dim,
        cond_mode='text',
        fsq_levels=tokenizer.fsq_levels,
    )
    ckpt_path = pjoin(model_dir, 'latest.tar')
    checkpoint = torch.load(ckpt_path, map_location='cpu')
    missing_keys, unexpected_keys = ema_isodim.load_state_dict(checkpoint['ema_isodim'], strict=False)
    assert len(unexpected_keys) == 0
    assert all([k.startswith('clip_model.') for k in missing_keys])
    
    print(f"\n{'='*60}")
    print(f"Evaluating IsoDiM: {args.name}")
    print(f"  Model: {args.model}")
    print(f"  Tokenizer: {args.tokenizer_name}")
    print(f"  FSQ Dimension: {tokenizer.fsq_dim}")
    print(f"  Codebook Size: {tokenizer.codebook_size:,}")
    print(f"  Timesteps: {args.time_steps}")
    print(f"  CFG Scale: {args.cfg}")
    print(f"{'='*60}\n")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    eval_wrapper = Evaluators(args.dataset_name, device=device)
    
    # =========================================================================
    # Evaluation
    # =========================================================================
    out_dir = pjoin(args.checkpoints_dir, args.dataset_name, args.name, 'eval')
    os.makedirs(out_dir, exist_ok=True)
    f = open(pjoin(out_dir, 'eval.log'), 'w')
    
    tokenizer.eval()
    tokenizer.to(device)
    ema_isodim.eval()
    ema_isodim.to(device)
    
    fid = []
    div = []
    top1 = []
    top2 = []
    top3 = []
    matching = []
    mm = []
    clip_scores = []
    
    repeat_time = 20
    for i in range(repeat_time):
        with torch.no_grad():
            best_fid, best_div, best_top1, best_top2, best_top3, best_matching, best_mm, clip_score = 1000, 0, 0, 0, 0, 100, 0, -1
            best_fid, best_div, best_top1, best_top2, best_top3, best_matching, best_mm, clip_score, writer, save_now = evaluation_isodim(
                model_dir, eval_loader, ema_isodim, tokenizer, None, i,
                best_fid=best_fid, clip_score_old=clip_score,
                best_div=best_div, best_top1=best_top1, best_top2=best_top2, best_top3=best_top3,
                best_matching=best_matching, eval_wrapper=eval_wrapper,
                device=device, train_mean=mean, train_std=std,
                time_steps=args.time_steps, cond_scale=args.cfg, temperature=args.temperature,
                cal_mm=args.cal_mm, draw=False, hard_pseudo_reorder=args.hard_pseudo_reorder
            )
        fid.append(best_fid)
        div.append(best_div)
        top1.append(best_top1)
        top2.append(best_top2)
        top3.append(best_top3)
        matching.append(best_matching)
        mm.append(best_mm)
        clip_scores.append(clip_score)
    
    fid = np.array(fid)
    div = np.array(div)
    top1 = np.array(top1)
    top2 = np.array(top2)
    top3 = np.array(top3)
    matching = np.array(matching)
    mm = np.array(mm)
    clip_scores = np.array(clip_scores)
    
    print(f'\nFinal Results:')
    print(f'Final Results:', file=f, flush=True)
    
    msg_final = f"\tFID: {np.mean(fid):.3f}, conf. {np.std(fid) * 1.96 / np.sqrt(repeat_time):.3f}\n" \
                f"\tDiversity: {np.mean(div):.3f}, conf. {np.std(div) * 1.96 / np.sqrt(repeat_time):.3f}\n" \
                f"\tTOP1: {np.mean(top1):.3f}, conf. {np.std(top1) * 1.96 / np.sqrt(repeat_time):.3f}, " \
                f"TOP2: {np.mean(top2):.3f}, conf. {np.std(top2) * 1.96 / np.sqrt(repeat_time):.3f}, " \
                f"TOP3: {np.mean(top3):.3f}, conf. {np.std(top3) * 1.96 / np.sqrt(repeat_time):.3f}\n" \
                f"\tMatching: {np.mean(matching):.3f}, conf. {np.std(matching) * 1.96 / np.sqrt(repeat_time):.3f}\n" \
                f"\tMultimodality: {np.mean(mm):.3f}, conf. {np.std(mm) * 1.96 / np.sqrt(repeat_time):.3f}\n" \
                f"\tCLIP-Score: {np.mean(clip_scores):.3f}, conf. {np.std(clip_scores) * 1.96 / np.sqrt(repeat_time):.3f}\n"
    
    print(msg_final)
    print(msg_final, file=f, flush=True)
    f.close()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Evaluate IsoDiM Model")
    
    # Model configuration
    parser.add_argument('--name', type=str, default='IsoDiM',
                        help='IsoDiM checkpoint name')
    parser.add_argument('--tokenizer_name', type=str, default='IsoDiM_Tokenizer_High',
                        help='Tokenizer checkpoint name')
    parser.add_argument('--tokenizer_model', type=str, default='IsoDiM_Tokenizer_High',
                        choices=['IsoDiM_Tokenizer_High', 'IsoDiM_Tokenizer_Ultra', 'IsoDiM_Tokenizer_Large'],
                        help='Tokenizer model variant')
    parser.add_argument('--model', type=str, default='IsoDiM-DiT-XL',
                        choices=['IsoDiM-DiT-S', 'IsoDiM-DiT-B', 'IsoDiM-DiT-L', 'IsoDiM-DiT-XL'],
                        help='IsoDiM model variant')
    
    # Data configuration
    parser.add_argument('--dataset_name', type=str, default='t2m',
                        choices=['t2m', 'kit'])
    parser.add_argument('--dataset_dir', type=str, default='./datasets')
    
    # Sampling configuration
    parser.add_argument("--time_steps", default=18, type=int,
                        help='Number of diffusion sampling steps')
    parser.add_argument("--cfg", default=4.5, type=float,
                        help='Classifier-free guidance scale')
    parser.add_argument("--temperature", default=1.0, type=float,
                        help='Sampling temperature')
    parser.add_argument('--cal_mm', action="store_false",
                        help='Calculate multimodality')
    parser.add_argument('--hard_pseudo_reorder', action="store_true",
                        help='Use hard pseudo reorder')
    
    # System configuration
    parser.add_argument("--seed", type=int, default=3407)
    parser.add_argument("--num_workers", type=int, default=4)
    parser.add_argument('--checkpoints_dir', type=str, default='./checkpoints')
    
    args = parser.parse_args()
    main(args)

